import re

import streamlit as st
import json, os, shutil, openai, csv
from pathlib import Path
from tqdm import tqdm
from dotenv import load_dotenv
from langchain.evaluation.qa import QAGenerateChain
from langchain.vectorstores import FAISS, Qdrant
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI, AzureChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.embeddings import (
    AzureOpenAIEmbeddings,
    HuggingFaceEmbeddings,
)
from langchain.document_loaders import (
    PyMuPDFLoader,
    TextLoader,
)
from langchain.retrievers import (
    SVMRetriever,
    AzureCognitiveSearchRetriever,
    TFIDFRetriever,
    ContextualCompressionRetriever,
)
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.prompts import PromptTemplate
import pandas as pd
from langchain.evaluation.qa import QAEvalChain, CotQAEvalChain
from evaluate import load, combine
from langchain.output_parsers import StructuredOutputParser, ResponseSchema

from src.html_preprocess import get_document_text, split_text_html
from langchain.docstore.document import Document

work_path = os.path.abspath('.')

def setup_env():
    # Load OpenAI key
    if os.path.exists("key.txt"):
        shutil.copyfile("key.txt", ".env")
        load_dotenv()
    else:
        print("key.txt with OpenAI API is required")

    # Load config values
    if os.path.exists(os.path.join(r'config.json')):
        with open(r'config.json') as config_file:
            config_details = json.load(config_file)

        # Setting up the embedding model
        embedding_model_name = config_details['EMBEDDING_MODEL']
        openai.api_type = "azure"
        openai.api_base = config_details['OPENAI_API_BASE']
        openai.api_version = config_details['OPENAI_API_VERSION']
        openai.api_key = os.getenv("OPENAI_API_KEY")

        # Aure Cognitive Search
        os.environ["AZURE_COGNITIVE_SEARCH_SERVICE_NAME"] = config_details['AZURE_COGNITIVE_SEARCH_SERVICE_NAME']
        os.environ["AZURE_COGNITIVE_SEARCH_INDEX_NAME"] = config_details['AZURE_COGNITIVE_SEARCH_INDEX_NAME']
        os.environ["AZURE_COGNITIVE_SEARCH_API_KEY"] = os.getenv('AZURE_COGNITIVE_SEARCH_API_KEY')
    else:
        print("config.json with Azure OpenAI config is required")


def set_reload_setting_flag():
    # st.write("New document need upload")
    st.session_state["evalreloadflag"] = True


def define_llm(model: str, max_token: int):
    if "gpt-" in model:
        llm = AzureChatOpenAI(deployment_name=model,
                              openai_api_key=openai.api_key,
                              azure_endpoint=openai.api_base,
                              openai_api_type=openai.api_type,
                              openai_api_version=openai.api_version,
                              max_tokens=max_token,
                              temperature=0.2,
                              # model_kwargs={'engine': self.config_details['CHATGPT_MODEL']},
                              )
    elif "" in model:
        pass
    return llm


def define_retriver(retriver: str):
    if retriver == "Similarity Search":
        pass
    elif retriver == "Azure Cognitive Search":
        pass
    elif retriver == "SVM":
        pass
    return retriver


def define_splitter(splitter: str, chunk_size, chunk_overlap):
    text_splitter = None
    if splitter == "RecursiveTextSplitter":
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    elif splitter == "CharacterTextSplitter":
        text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return text_splitter


def define_embedding(embedding_method: str):
    embeddings = None
    if embedding_method == "OpenAI":
        embeddings = AzureOpenAIEmbeddings(deployment="text-embedding-ada-002",
                                      azure_endpoint=openai.api_base,
                                      openai_api_type=openai.api_type,
                                      chunk_size=1,)
    elif embedding_method == "HuggingFace":
        embeddings = HuggingFaceEmbeddings()
    return embeddings


def load_single_document(file_path):
    loader = PyMuPDFLoader(file_path)
    return loader.load()  # [0]

def save_csv(examples, filename:str="generated_QA.csv"):
    df = pd.DataFrame(examples)
    df.to_csv(filename)

def show_csv(examples):
    df = pd.DataFrame(examples)
    st.dataframe(df)
    return df

def eval_metric_squad(new_examples, predictions):
    squad_metric = load("squad")
    results = []
    for i in range(len(new_examples)):
        results.append(squad_metric.compute(
            references=[new_examples[i]],
            predictions=[predictions[i]],
        ))
    return results

def eval_metric_rouge(new_examples, predictions):
    rouge_metric = load('rouge')
    rouge_results = []
    for i in range(len(new_examples)):
        rouge_results.append(rouge_metric.compute(
            references=new_examples[i]["answers"]["text"],
            predictions=[predictions[i]["prediction_text"]],
        ))
    return rouge_results

def main():
    # Initial
    setup_env()
    if "evalreloadflag" not in st.session_state:
        st.session_state["evalreloadflag"] = True
    if "EvalTexts" not in st.session_state:
        st.session_state["EvalTexts"] = None
    if "EvalQAs" not in st.session_state:
        st.session_state["EvalQAs"] = None
    if "EvalQAChain" not in st.session_state:
        st.session_state["EvalQAChain"] = None
    if "EvalUploadFile" not in st.session_state:
        st.session_state["EvalUploadFile"] = None
    if "EvalQAresults" not in st.session_state:
        st.session_state["EvalQAresults"] = None

    # Setup Side Bar
    with st.sidebar:
        # 1. Model
        aa_llm_model = st.radio(label="`LLM Model`",
                                options=["gpt-35-turbo", "gpt-35-turbo-16k", "gpt-4", "gpt-4-turbo"],
                                index=0,
                                on_change=set_reload_setting_flag)
        aa_max_token = st.slider(label="`Max token`",
                                 min_value=1024,
                                 max_value=16*1024,
                                 value=4096,
                                 on_change=set_reload_setting_flag)
        # 2. Split
        aa_eval_q = st.slider(label="`Number of eval questions`",
                              min_value=1,
                              max_value=100,
                              value=50,
                              on_change=set_reload_setting_flag)
        aa_chunk_size = st.slider(label="`Choose chunk size for splitting`",
                                  min_value=500,
                                  max_value=2000,
                                  value=1000,
                                  on_change=set_reload_setting_flag)
        aa_overlap_size = st.slider(label="`Choose overlap for splitting`",
                                    min_value=0,
                                    max_value=200,
                                    value=100,
                                    on_change=set_reload_setting_flag)
        aa_split_methods = st.radio(label="`Split method`",
                                    options=["RecursiveTextSplitter", "CharacterTextSplitter"],
                                    index=0,
                                    on_change=set_reload_setting_flag)

        # 3. Retriver
        aa_vector_store = st.radio(label="`Choose vector store`",
                                   options=["FAISS", "Qdrant"],
                                   index=0,
                                   on_change=set_reload_setting_flag)
        aa_retriver = st.radio(label="`Choose retriever`",
                               options=["Similarity Search", "MMR", "Contextual Compression","Azure Cognitive Search", "SVM", "TFIDF"],
                               index=0,
                               on_change=set_reload_setting_flag)
        aa_chunk_num = st.select_slider("`Choose # chunks to retrieve`",
                                        options=[3, 4, 5, 6, 7, 8],
                                        on_change=set_reload_setting_flag)

        # 4. Embedding
        aa_embedding_method = st.radio(label="`Choose embeddings`",
                                       options=["OpenAI", "HuggingFace"],
                                       index=0,
                                       on_change=set_reload_setting_flag)

        # if st.session_state["evalreloadflag"] == True:
        LlmModel = define_llm(aa_llm_model, aa_max_token)
        EmbeddingModel = define_embedding(aa_embedding_method)
        TextSplitter = define_splitter(aa_split_methods, aa_chunk_size, aa_overlap_size)
        st.session_state["evalreloadflag"] = False

    ## Main
    st.header("`Demo auto-evaluator`")
    qa_gen_container = st.container()
    # st.write("2. Upload QA pairs")
    qa_upload_container = st.container()
    st.write("3. Run EVAL")
    eval_container = st.container()

    with qa_gen_container:
        # Upload file to generate Q&A pairs
        file_paths = st.file_uploader("1.Upload document files to generate QAs",
                                      type=["pdf", "html"],
                                      accept_multiple_files=True)

        if st.button("Upload Ref Document", type="primary"):
            if file_paths is not None or len(file_paths) > 0:
                # save file
                with st.spinner('Reading file'):
                    uploaded_paths = []

                    # Ensure tempDir exists
                    tempOutputDir = work_path + "/tempDir/output"
                    if not os.path.exists(tempOutputDir):
                        os.makedirs(tempOutputDir)

                    for file_path in file_paths:
                        uploaded_paths.append(os.path.join(tempOutputDir, file_path.name))
                        uploaded_path = uploaded_paths[-1]
                        with open(uploaded_path, mode="wb") as f:
                            f.write(file_path.getbuffer())

                # process file
                with st.spinner('Create vector DB'):
                    # load documents
                    documents = []
                    with tqdm(total=len(uploaded_paths), desc='Loading new documents', ncols=80) as pbar:
                        for uploaded_path in uploaded_paths:
                            # file is pdf
                            if Path(uploaded_path).suffix == ".pdf":
                                documents = documents + load_single_document(uploaded_path)
                            elif Path(uploaded_path).suffix == ".html":
                                page_map = get_document_text(uploaded_path)
                                for i, (content, pagenum) in enumerate(split_text_html(page_map)):
                                    doc = Document(page_content=content, metadata={"source": uploaded_path})
                                    documents = documents + [doc]

                            pbar.update()

                    # split documents
                    # for i in range(len(uploaded_paths)):
                    uploaded_path = uploaded_paths[0]
                    texts = TextSplitter.split_documents(documents)
                    #texts = documents
                    st.session_state["EvalTexts"] = texts

                    # search & retriver
                    # FAISS/Qdrant: save documents as index, and then load them(not use the save function)
                    # Others do not support save for now
                    if aa_vector_store == "FAISS":
                        single_index_name = "./index/" + Path(uploaded_path).stem + ".faiss"
                        if Path(single_index_name).is_file() == False:
                            tmpdb = FAISS.from_documents(texts, EmbeddingModel)
                            # tmpdocsearch = tmpdb.as_retriever(search_kwargs={"k": aa_chunk_num})
                            tmpdb.save_local("./index/", Path(uploaded_path).stem)
                        else:
                            tmpdb = FAISS.load_local("./index/", EmbeddingModel, Path(uploaded_path).stem)
                    elif aa_vector_store == "Qdrant":
                        single_index_name = "./index/" + Path(uploaded_path).stem + ".qdrant"
                        tmpdb = Qdrant.from_documents(texts,
                                                      EmbeddingModel,
                                                      path="./index/",
                                                      collection_name=Path(uploaded_path).stem,
                                                      on_disk_payload=True,
                                                      force_recreate=False
                                                      )

                    if aa_retriver == "Similarity Search":
                        # if Path(single_index_name).is_file() == False:
                        #     tmpdb = FAISS.from_documents(texts, EmbeddingModel)
                        #     # tmpdocsearch = tmpdb.as_retriever(search_kwargs={"k": aa_chunk_num})
                        #     tmpdb.save_local("./index/", Path(uploaded_path).stem)
                        # else:
                        #     tmpdb = FAISS.load_local("./index/", EmbeddingModel, Path(uploaded_path).stem)
                        tmpdocsearch = tmpdb.as_retriever(search_kwargs={"k": aa_chunk_num}) # default is "similarity"
                    elif aa_retriver == "MMR":
                        # like "Similarity Search" but fetch more splits and return a more diversity smaller splits
                        # if Path(single_index_name).is_file() == False:
                        #     tmpdb = FAISS.from_documents(texts, EmbeddingModel)
                        #     # tmpdocsearch = tmpdb.as_retriever(search_kwargs={"k": aa_chunk_num})
                        #     tmpdb.save_local("./index/", Path(uploaded_path).stem)
                        # else:
                        #     tmpdb = FAISS.load_local("./index/", EmbeddingModel, Path(uploaded_path).stem)
                        tmpdocsearch = tmpdb.as_retriever(search_type="mmr", search_kwargs={"k": aa_chunk_num}) # default fetch_k = 20
                    elif aa_retriver == "Contextual Compression":
                        # find the splits and compress them, then return only the relevant info
                        # if Path(single_index_name).is_file() == False:
                        #     tmpdb = FAISS.from_documents(texts, EmbeddingModel)
                        #     # tmpdocsearch = tmpdb.as_retriever(search_kwargs={"k": aa_chunk_num})
                        #     tmpdb.save_local("./index/", Path(uploaded_path).stem)
                        # else:
                        #     tmpdb = FAISS.load_local("./index/", EmbeddingModel, Path(uploaded_path).stem)
                        compressor = LLMChainExtractor.from_llm(LlmModel)
                        tmpdocsearch = ContextualCompressionRetriever(
                            base_compressor=compressor,
                            base_retriever=tmpdb.as_retriever(search_type="mmr")
                        )
                    elif aa_retriver == "SVM":
                        tmpdocsearch = SVMRetriever.from_documents(texts, EmbeddingModel, k=aa_chunk_num)
                    elif aa_retriver == "TFIDF":
                        tmpdocsearch = TFIDFRetriever.from_documents(texts, k=aa_chunk_num)
                    elif aa_retriver == "Azure Cognitive Search":
                        tmpdocsearch = AzureCognitiveSearchRetriever(content_key="content", top_k=aa_chunk_num)

                    docsearch = tmpdocsearch

                    # make chain
                    # qa_chain = RetrievalQA.from_chain_type(LlmModel, retriever=docsearch)
                    qa_chain = RetrievalQA.from_llm(llm=LlmModel, retriever=docsearch)
                    st.session_state["EvalQAChain"] = qa_chain

                    if len(uploaded_paths) > 0:
                        st.session_state["EvalUploadFile"] = Path(uploaded_path).stem
                        st.write(f"✅ " + ", ".join(uploaded_paths) + " uploaed")

        # Prompt for QA generation
        template_QAGen = """You are a teacher coming up with questions to ask on a quiz.
        Use the following process to generate question and answer pairs. 
        1) Search the document snippet for specific, concrete facts or concepts. 
        2) construct a concise statement of the fact or concept.
        3) construct a question for which the concise statement is the answer. 
        Finally, output the question and answer in the following format:
        <Begin Document>
        ...
        <End Document>
        QUESTION: question here
        ANSWER: answer here

        Please iteratively search for concrete, specific facts or concepts and keep iterating and generating until you can't find any more concrete facts or concepts. 
        These questions should be detailed and be based explicitly on information in the document. Begin!

        <Begin Document>
        {doc}
        <End Document>"""

# """
# You are a teacher coming up with questions to ask on a quiz.
# Use the following process to generate question and answer pairs.
# 1) Search the document snippet for specific, concrete facts or concepts.
# 2) construct a concise statement of the fact or concept.
# 3) construct a very hard and comprehensive question-answer pair based on the concise statements.
# Finally, output the question and answer in the following format:
# <Begin Document>
# ...
# <End Document>
# QUESTION: question here
# ANSWER: answer here
#
# Please create one question-answer pair per chuck.
# These questions should be detailed and be based explicitly on information in the document. Begin!
#
# <Begin Document>
# {doc}
# <End Document>
# """

        input_QAGen = st.text_area(label="QA Generation Prompt", value=template_QAGen, )
        PROMPT_QAGen = PromptTemplate(
            input_variables=["doc"],
            template=input_QAGen,
        )
        # Generate Q&A
        if st.button("Generate Q&A"):
            with st.spinner('Generating QnA pairs...'):
                texts = st.session_state["EvalTexts"]
                examples = []
                use_custom_prompt = True if input_QAGen != "" else False
                # # Hard - coded examples
                # examples = [
                #     {
                #         "query": "What did the president say about Ketanji Brown Jackson",
                #         "answer": "He praised her legal ability and said he nominated her for the supreme court.",
                #     },
                #     {"query": "What did the president say about Michael Jackson", "answer": "Nothing"},
                # ]
                if use_custom_prompt == False:
                    example_gen_chain = QAGenerateChain.from_llm(LlmModel)
                    new_examples = example_gen_chain.apply_and_parse([{"doc": t} for t in texts[:aa_eval_q]])
                else:
                    example_gen_chain = QAGenerateChain.from_llm(LlmModel, prompt=PROMPT_QAGen)
                    outputs = example_gen_chain.generate([{"doc": t} for t in texts[:aa_eval_q]]) #.apply([{"doc": t} for t in texts[:aa_eval_q]])

                    output_keys = ["query", "answer"]
                    regex = r"QUESTION: (.*?)\n+ANSWER: (.*)"
                    output_parser = []
                    for generation in outputs.generations:
                        tmptext = generation[0].text
                        match = re.findall(regex, tmptext)
                        if match:
                            # output_parser = []
                            for item in match:
                                output_parser.append({"qa_pairs": {key: item[i] for i, key in enumerate(output_keys)}})
                        else:
                            raise ValueError(f"Could not parse output: {tmptext}")
                    # print(output_parser.parse(outputs.generations[0]))
                    # new_examples = example_gen_chain.apply_and_parse([{"doc": t} for t in texts[:aa_eval_q]])
                    new_examples = output_parser
                # print(new_examples[0])

                # Combine examples
                examples += [tmp["qa_pairs"] for tmp in new_examples]
                st.session_state["EvalQAs"] = examples
        if st.session_state["EvalQAs"] is not None:
            df = show_csv(st.session_state["EvalQAs"])
            # save_csv(examples)
            # export srt file
            csv_QA = df.to_csv(index=False)
            tmpQAfile = st.session_state["EvalUploadFile"]
            st.download_button("Download .csv file", data=csv_QA, file_name=f"{tmpQAfile}_QA_autogen_pairs.csv")

    # upload QA pairs
    with qa_upload_container:
        # Upload QA file to EVAL
        file_paths = [st.file_uploader("2.Upload ground TRUE QAs", type=["csv"], accept_multiple_files=False)]
        if st.button("Upload QnA pairs", type="primary"):
            if file_paths is not None or len(file_paths) > 0:
                # load csv file to list of dict
                with st.spinner('Reading QnA pairs file'):
                    list_dicts = []
                    for file_path in file_paths:
                        upload_qa_df = pd.read_csv(file_path)
                        list_dicts += upload_qa_df.to_dict("records")
                    st.session_state["EvalUploadFile"] = Path(file_path.name).stem
                    st.session_state["EvalQAs"] = list_dicts
                    # print(st.session_state["EvalQAs"])

    with eval_container:
        # Prompt for evaluation
        template = """You are a teacher grading a quiz.
        You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.
        Write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.

        Example Format:
        QUESTION: question here
        TRUE ANSWER: true answer here
        STUDENT ANSWER: student's answer here
        GRADE: CORRECT or INCORRECT here

        Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing 
        between the student answer and true answer. It is OK if the student answer contains more information than the true answer, 
        as long as it does not contain any conflicting statements. Begin! 

        QUESTION: {query}
        TRUE ANSWER: {answer}
        STUDENT ANSWER: {result}
        GRADE:"""
        input = st.text_area(label="Evaluation Prompt", value=template, )
        PROMPT = PromptTemplate(
            input_variables=["query", "answer", "result"], template=input
        )
        # Start EVAL
        if st.button("Start EVAL", type="primary"):
            with st.spinner('Evaluating the LLM setting...'):
                if st.session_state["EvalQAs"] is not None:
                    examples = st.session_state["EvalQAs"]
                else:
                    st.toast("No QnA pairs are uploaded or generated!")
                if st.session_state["EvalQAChain"] is not None:
                    qa_chain = st.session_state["EvalQAChain"]
                else:
                    st.toast("No qa chain is created!")

                predictions = qa_chain.apply(examples)
                eval_chain = QAEvalChain.from_llm(LlmModel, prompt=PROMPT)
                # eval_chain = CotQAEvalChain.from_llm(llm2)
                graded_outputs = eval_chain.evaluate(examples, predictions)
                # for i, eg in enumerate(examples[:3]):
                #     print(f"Example {i}:")
                #     print("Question: " + predictions[i]["query"])
                #     print("Real Answer: " + predictions[i]["answer"])
                #     print("Predicted Answer: " + predictions[i]["result"])
                #     print("Predicted Grade: " + graded_outputs[i]["results"])
                #     print()

                # output with binary eval
                outputs = [
                    {
                        "query": predictions[i]["query"],
                        "answer": predictions[i]["answer"],
                        "predict result": predictions[i]["result"],
                        "grade": graded_outputs[i]["results"]
                    }
                    for i, example in enumerate(examples)
                ]
                # show_csv(outputs)
                # save_csv(outputs, "grade_result.csv")

                # perform SQUaD Eval
                # Some data munging to get the examples in the right format
                for i, eg in enumerate(examples):
                    eg["id"] = str(i)
                    eg["answers"] = {"text": [eg["answer"]], "answer_start": [0]}
                    predictions[i]["id"] = str(i)
                    predictions[i]["prediction_text"] = predictions[i]["result"]

                for p in predictions:
                    del p["result"]
                    del p["query"]
                    del p["answer"]
                    # del p["text"]

                new_examples = examples.copy()
                for eg in new_examples:
                    del eg["query"]
                    del eg["answer"]

                # squad_metric = load("squad")
                # results = []
                # for i in range(len(new_examples)):
                #     results.append(squad_metric.compute(
                #         references=[new_examples[i]],
                #         predictions=[predictions[i]],
                #     ))
                results = eval_metric_squad(new_examples, predictions)
                print(results)

                # # https://huggingface.co/spaces/evaluate-metric/rouge
                # rouge_metric = load('rouge')
                # rouge_results = []
                # for i in range(len(new_examples)):
                #     rouge_results.append(rouge_metric.compute(
                #         references=new_examples[i]["answers"]["text"],
                #         predictions=[predictions[i]["prediction_text"]],
                #     ))
                #     print(rouge_results[i]["rouge1"])
                rouge_results = eval_metric_rouge(new_examples, predictions)

                new_outputs = [
                    {
                        "query": outputs[i]["query"],
                        "answer": outputs[i]["answer"],
                        "predict result": outputs[i]["predict result"],
                        "grade": outputs[i]["grade"],
                        "exact_match": results[i]["exact_match"],
                        "f1": results[i]["f1"],
                        "rouge1": rouge_results[i]["rouge1"]
                    }
                    for i, example in enumerate(outputs)
                ]

                st.session_state["EvalQAresults"] = new_outputs

        if st.session_state["EvalQAresults"] is not None:
            df = show_csv(st.session_state["EvalQAresults"])
            # save_csv(new_outputs, "f1_grade_result.csv")
            csv_eval = df.to_csv()
            tmpEVALfile = st.session_state["EvalUploadFile"]
            st.download_button("Download EVAL file", data=csv_eval, file_name=f"{tmpEVALfile}_EVAL.csv")

if __name__ == "__main__":
    main()
